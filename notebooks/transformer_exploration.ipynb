{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YSPXFloQLxO"
   },
   "source": [
    "# Transformer\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n",
    "Este notebook tiene como finalidad implementar y explicar un modelo Transformer básico utilizando PyTorch, desde una perspectiva educativa y conceptual.\n",
    "\n",
    "A diferencia del análisis comparativo entre RNN y LSTM (donde se evaluó directamente el desempeño en una tarea de clasificación de sentimiento), aquí el enfoque es comprender los **componentes clave** de la arquitectura Transformer, tales como:\n",
    "\n",
    "- El mecanismo de atención\n",
    "- Las capas encoder\n",
    "- La codificación posicional\n",
    "- El entrenamiento en paralelo\n",
    "\n",
    "## Motivación\n",
    "\n",
    "Aunque el Transformer se ha consolidado como el estándar en tareas avanzadas de NLP (como BERT, GPT, T5, etc.), construir uno desde cero permite entender **cómo y por qué** esta arquitectura supera a las redes recurrentes en muchos contextos.\n",
    "\n",
    "> ⚠️ Nota: Este notebook no busca superar a RNN o LSTM en métricas de clasificación, sino ilustrar los fundamentos de una arquitectura moderna y liviana de Transformer aplicada a texto.\n",
    "\n",
    "## Herramientas utilizadas\n",
    "- PyTorch para la definición del modelo\n",
    "- NLTK para tokenización\n",
    "- Dataset de Sentiment140 (mismo que en los otros modelos)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvJkrIR3gPn_"
   },
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxT6hyOngUP5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "#BLEU\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LtP9MkeHhtmf",
    "outputId": "9d393c41-5362-4c9d-94dc-ba71ddd01fc5"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57DAlIxSqXaR"
   },
   "source": [
    "## Arquitectura Transformer: Componentes Clave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UqFB3d8qfFd"
   },
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCuo44jUrLEw"
   },
   "source": [
    "El modelo Transformer, introducido por Vaswani et al. en 2017, es una arquitectura de deep learning que ha revolucionado el procesamiento de lenguaje natural (NLP). A diferencia de las redes recurrentes (RNN), los Transformers permiten el procesamiento paralelo y capturan dependencias a largo plazo de manera eficiente gracias al mecanismo de atención."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALrgJc6Iqi4k"
   },
   "source": [
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZl58NQlrIF_"
   },
   "source": [
    "El modelo Transformer sigue una arquitectura de **Encoder-Decoder**, donde:\n",
    "\n",
    "- **Encoder**: Procesa la secuencia de entrada (`input sentence`) y genera una representación contextualizada.\n",
    "- **Decoder**: Toma la representación del encoder y genera la secuencia de salida (`target sentence`), paso a paso.\n",
    "\n",
    "Cada uno se compone de múltiples capas idénticas (stacked).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtdiBBbbgtVr"
   },
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFEFSAxHgsk1"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo3VJDsqktb2"
   },
   "source": [
    "### Positional Encoding en el Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv4LYE78kvh3"
   },
   "source": [
    "En los modelos `Transformer`, a diferencia de los modelos RNN o LSTM, no tiene alguna forma o mecanismo natural para procesar las secuencias en orden temporal paso a paso. Esto se debe a que la atención (attention) opera sobre toda la secuencia al mismo tiempo y por lo tanto no sabe quién va primero y después.\n",
    "\n",
    "Para poder solucionar esto, se incorpora explícitamente un `Positional Encoding`, el cual añade información de la posición relativa o absoluta de cada token dentro de la secuencia.\n",
    "\n",
    "Esta codificación de posición se suima a los embeddings de las palabras, de forma que el modelo pueda inferir el orden en el que aparecen las palabras.\n",
    "\n",
    "En el código implementado, el Positional Encoding funciona de la siguiente manera:\n",
    "\n",
    "  * Se crea un vector de posiciones (0, 1, 2, 3 ...).\n",
    "\n",
    "  * Para cada posición, se aplica una combinación de funciones seno y coseno con diferentes frecuencias.\n",
    "\n",
    "  * Estas funciones trigonométricas generan un patrón de valores que se repite y facilita que el modelo aprenda relaciones de dependencia entre tokens cercanos o lejanos.\n",
    "\n",
    "  * El resultado es una matriz del mismo tamaño que los embeddings, que se suma elemento a elemento.\n",
    "\n",
    "  * Así, cada embedding no solo contiene la representación semántica de la palabra, sino también su posición en la frase.\n",
    "\n",
    "Esto permite al modelo:\n",
    "\n",
    "  * Diferenciar el primer token del último.\n",
    "\n",
    "  * Aprender relaciones de largo plazo.\n",
    "\n",
    "  * Aprovechar la atención de forma más eficiente.\n",
    "\n",
    "**Motivo de usar funciones seno/coseno**\n",
    "\n",
    "  * Estas funciones periódicas permiten representar posiciones de manera continua e interpolable.\n",
    "\n",
    "  * Son invariantes a traslaciones y ayudan a generalizar a secuencias de longitud diferente.\n",
    "\n",
    "  * Además, no requieren ser aprendidas (a diferencia de un embedding de posición entrenable), reduciendo parámetros y mejorando estabilidad.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeKawdArg1qM"
   },
   "source": [
    "## Transformer Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgXrpJuXg73G"
   },
   "outputs": [],
   "source": [
    "class TransformerTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4, dim_feedforward=512, max_len=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None):\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        output = self.transformer(\n",
    "            src=src, tgt=tgt,\n",
    "            src_mask=src_mask, tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask\n",
    "        )\n",
    "        return self.output_layer(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8drZf_5nOya"
   },
   "source": [
    "### Arquitectura TransformerLM para generación de texto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnPW60zMrjWO"
   },
   "source": [
    "\n",
    "La clase `TransformerLM` implementa un modelo de lenguaje basado en el bloque Transformer Encoder, diseñado para generación de texto de forma secuencial.\n",
    "Los componentes del código funcionan de la siguiente forma:\n",
    "\n",
    "  * **Embedding**  \n",
    "  Se encarga de transformar cada token (representado por un índice entero) en un vector de dimensión `d_model`. De este modo, cada palabra obtiene una representación semántica densa y continua que puede ser procesada por la red.\n",
    "\n",
    "  * **Positional Encoding**  \n",
    "  Como el Transformer no procesa la secuencia en orden explícito, se incorpora una codificación de posición, utilizando funciones seno/coseno, para darle al modelo una noción del orden de las palabras dentro de la frase.\n",
    "\n",
    "  * **Transformer Encoder**  \n",
    "  Utiliza múltiples capas (`num_layers`) de bloques de encoder compuestos de:\n",
    "\n",
    "    * Multi-head self-attention\n",
    "\n",
    "    * Feedforward\n",
    "\n",
    "    * Normalización y residual connections\n",
    "\n",
    "  Este mecanismo le permite capturar dependencias de largo plazo y modelar relaciones entre palabras de manera no secuencial.  \n",
    "  La atención multi-cabeza facilita que el modelo combine diferentes perspectivas de contexto, con varias “cabezas” especializadas en distintas relaciones dentro de la frase.\n",
    "\n",
    "- **Máscara causal (src_mask)**  \n",
    "  Durante la generación de texto, se necesita evitar que el modelo “vea” el futuro. Para ello se aplica una máscara triangular que bloquea posiciones futuras, forzando a predecir la siguiente palabra solo a partir del contexto anterior. Esto es equivalente al comportamiento autoregresivo de un modelo de lenguaje clásico.\n",
    "\n",
    "- **Capa de salida (Linear)**  \n",
    "  Finalmente, se proyecta la salida del Transformer Encoder hacia el tamaño del vocabulario, generando para cada posición una distribución de probabilidad sobre el siguiente token posible.\n",
    "\n",
    "- **Shape management**  \n",
    "  El modelo transpone los tensores (usando `transpose`) para cumplir con el formato requerido por `nn.TransformerEncoder`, donde la dimensión de la secuencia va primero. Al final se vuelve a transponer para dejar el resultado en el formato tradicional de lotes (batch_size, seq_len, vocab_size).\n",
    "\n",
    "**Motivo de la arquitectura**  \n",
    "La arquitectura `TransformerLM` permite generar texto palabra por palabra (next-word prediction), capturando contextos complejos y dependencias de largo plazo. Esto ofrece ventajas notables respecto a arquitecturas RNN tradicionales, como:\n",
    "\n",
    "  * mayor paralelización\n",
    "\n",
    "  * mejor capacidad de aprender relaciones distantes\n",
    "  \n",
    "  * mayor eficiencia computacional en secuencias largas\n",
    "\n",
    "En resumen, `TransformerLM` combina embeddings, codificación de posición, atención multi-cabeza y feedforward, constituyendo un modelo de lenguaje potente y flexible para tareas de NLP como la generación de texto.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdvFn3JVhAgP"
   },
   "source": [
    "## Generación de texto con máscara causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Txc4XbrJg_JM"
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones(sz, sz) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYCQBukQoMOe"
   },
   "source": [
    "### Implementación de la máscara causal para atención autoregresiva\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3s8mGiX4rlsg"
   },
   "source": [
    "\n",
    "En el modelo Transformer para generación de texto autoregresiva, es necesario garantizar que el modelo **no pueda ver palabras futuras** al momento de predecir la siguiente palabra. Esto equivale a forzar que cada posición solo tenga acceso a los tokens anteriores, tal como haría un modelo de lenguaje tradicional (por ejemplo un RNN).\n",
    "\n",
    "La función `generate_square_subsequent_mask(sz)` crea precisamente esta máscara triangular superior, también llamada **máscara causal**, para restringir la atención a los tokens pasados o actuales. Así se evita el *data leakage* durante el entrenamiento.\n",
    "\n",
    "**Explicación técnica:**\n",
    "- `torch.ones(sz, sz)` crea una matriz de unos  \n",
    "- `torch.triu(...).transpose(0,1)` genera la triangular superior transpuesta, dejando unos en las posiciones permitidas  \n",
    "- Luego se convierte a flotante y se rellena con `-inf` en las posiciones no permitidas, para que el softmax de la atención las ignore  \n",
    "- En las posiciones permitidas, se pone un 0 para no alterar la puntuación de atención  \n",
    "- Finalmente, la máscara se retorna con forma `(sz, sz)`\n",
    "\n",
    "**En resumen**, esta máscara causal impide que el modelo “espíe” tokens futuros, cumpliendo el principio autoregresivo fundamental en modelos de lenguaje basados en Transformer.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsrNQenshMQD"
   },
   "source": [
    "## Ejemplo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VE26R8L9hGw0",
    "outputId": "7b52c86c-a09c-404d-e939-79174bddbad7"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, src, vocab, inv_vocab, max_len=10, bos_token=1, eos_token=2):\n",
    "    model.eval()\n",
    "    generated = torch.tensor([[bos_token]], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            tgt_mask = generate_square_subsequent_mask(generated.size(1)).to(src.device)\n",
    "            out = model(src, generated, tgt_mask=tgt_mask)\n",
    "            next_token = out[:, -1, :].argmax(-1).unsqueeze(1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            if next_token.item() == eos_token:\n",
    "                break\n",
    "    return \" \".join(inv_vocab[idx.item()] for idx in generated[0][1:] if idx.item() in inv_vocab)\n",
    "\n",
    "# Vocabulario simple\n",
    "vocab = {\"<pad>\":0, \"<bos>\":1, \"<eos>\":2, \"I\":3, \"like\":4, \"cats\":5, \"dogs\":6}\n",
    "inv_vocab = {idx:tok for tok,idx in vocab.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Ejemplo de entrenamiento mínimo\n",
    "src = torch.tensor([[3, 4, 0]], dtype=torch.long)  # \"I like\"\n",
    "tgt_input = torch.tensor([[1, 5]], dtype=torch.long)  # \"<bos> cats\"\n",
    "tgt_output = torch.tensor([[5, 2]], dtype=torch.long)  # \"cats <eos>\"\n",
    "\n",
    "model = TransformerTextGenerator(vocab_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "tgt_mask = generate_square_subsequent_mask(tgt_input.size(1))\n",
    "\n",
    "# Entrenamiento simple\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, tgt_input, tgt_mask=tgt_mask)\n",
    "    loss = criterion(output.view(-1, vocab_size), tgt_output.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLi8sZHnvLlm"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'transformer_text_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJyHbagRhNbi"
   },
   "source": [
    "## Generar texto con el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngYOwfQYhUDQ",
    "outputId": "cc5bce53-54a5-4a64-c6f6-e2fdf3c2bcd3"
   },
   "outputs": [],
   "source": [
    "# Generar texto\n",
    "generated = generate_text(model, src, vocab, inv_vocab, max_len=5)\n",
    "print(\"Generated:\", generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CT7RfF8uiBmf"
   },
   "outputs": [],
   "source": [
    "reference = \"i like dogs\"\n",
    "candidate = \"i love dogs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDlvX_F1iDfE"
   },
   "outputs": [],
   "source": [
    "reference_tokens = nltk.word_tokenize(reference)\n",
    "candidate_tokens = nltk.word_tokenize(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSOXjO6hiIVI",
    "outputId": "7b714fa8-254c-43f0-f439-ff3d9af85ff4"
   },
   "outputs": [],
   "source": [
    "# BLEU espera una lista de referencias (en plural)\n",
    "bleu_score = sentence_bleu(\n",
    "    [reference_tokens],   # referencias (en plural)\n",
    "    candidate_tokens,     # hipótesis\n",
    "    smoothing_function=SmoothingFunction().method1  # suavizado\n",
    ")\n",
    "\n",
    "print(f\"BLEU score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wlx1yU0Zvu4T"
   },
   "source": [
    "## Análisis de resultados\n",
    "\n",
    "El modelo generó secuencias sintácticamente válidas como:\n",
    "\n",
    "> **Prompt:** I like  \n",
    "> **Output:** I like dogs cats like dogs\n",
    "\n",
    "Si bien repite palabras, respeta la estructura gramatical esperada y utiliza tokens del vocabulario.\n",
    "\n",
    "El **BLEU score fue 0.1351**, indicando coincidencia parcial con la referencia. Este valor es razonable considerando:\n",
    "\n",
    "- Vocabulario muy pequeño\n",
    "- Solo 1 oración de entrenamiento\n",
    "- Ausencia de embeddings preentrenados\n",
    "- Pocas épocas de entrenamiento\n",
    "\n",
    "## Posibles mejoras:\n",
    "\n",
    "- Usar un dataset real (como Wikitext o un corpus de diálogos)\n",
    "- Aumentar vocabulario y número de oraciones\n",
    "- Aumentar num_layers y d_model\n",
    "- Aplicar *beam search* en la generación en vez de greedy decoding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Este ejercicio permitió entender de forma práctica la estructura de un Transformer básico, resaltando su eficiencia, paralelismo y diseño modular. Para tareas de producción, es recomendable utilizar variantes preentrenadas como BERT, RoBERTa o DistilBERT.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
