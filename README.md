# ğŸ’¬ AnÃ¡lisis de Sentimiento con RNN, LSTM y ExploraciÃ³n de Transformer

Este proyecto aplica tÃ©cnicas de procesamiento de lenguaje natural (NLP) y aprendizaje profundo para clasificar sentimientos en tweets. El enfoque principal estÃ¡ en comparar dos arquitecturas clÃ¡sicas: **RNN (Recurrent Neural Network)** y **LSTM (Long Short-Term Memory)** usando el dataset **Sentiment140**.

Adicionalmente, se incluye una implementaciÃ³n educativa de un **Transformer bÃ¡sico** en PyTorch como experimento conceptual, sin fines comparativos directos.

---

## ğŸ“Š Dataset

Se utiliza el dataset [Sentiment140](http://help.sentiment140.com/), que contiene 1.6 millones de tweets etiquetados como positivos o negativos.

El dataset se descarga automÃ¡ticamente dentro del notebook usando `wget`, por lo que **no es necesario subirlo manualmente**.

> âš ï¸ En caso de falla con el enlace, puedes obtenerlo desde [Kaggle](https://www.kaggle.com/datasets/kazanova/sentiment140).

---

## ğŸ§  Modelos Desarrollados

### ğŸ” RNN vs LSTM (Keras, TensorFlow)
- Entrenamiento de ambas arquitecturas para clasificaciÃ³n binaria
- VisualizaciÃ³n y anÃ¡lisis de precisiÃ³n, F1-score y pÃ©rdida
- ComparaciÃ³n directa entre modelos para determinar el mÃ¡s efectivo

### ğŸ§ª Transformer (PyTorch) â€” *ExploraciÃ³n Conceptual*
- ImplementaciÃ³n desde cero basada en el paper original de Vaswani et al.
- ExplicaciÃ³n didÃ¡ctica de codificaciÃ³n posicional, atenciÃ³n y estructura encoder
- No se compara directamente con RNN o LSTM en mÃ©tricas

---

## ğŸ“ˆ Resultados: RNN vs LSTM

| Modelo     | PrecisiÃ³n | F1-Score | Tiempo de Entrenamiento | TamaÃ±o del Modelo |
|------------|-----------|----------|--------------------------|-------------------|
| RNN        | 72.9%     | 0.72     | ~12 min                  | 24.8 MB           |
| LSTM       | 77.0%     | 0.77     | ~15 min                  | 27.2 MB           |

> âœ… Se eligiÃ³ **LSTM** como modelo final por su mejor desempeÃ±o general y mayor estabilidad durante el entrenamiento.

---

## âš™ï¸ TecnologÃ­as utilizadas

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54) ![TensorFlow](https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=for-the-badge&logo=TensorFlow&logoColor=white) ![Keras](https://img.shields.io/badge/Keras-%23D00000.svg?style=for-the-badge&logo=Keras&logoColor=white) ![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white) ![scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=for-the-badge&logo=scikit-learn&logoColor=white) ![NLTK](https://img.shields.io/badge/NLTK-3776AB?style=for-the-badge&logo=python&logoColor=ffdd54) ![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white) ![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white) ![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=black)

[//]: # "- Python 3"
[//]: # "- TensorFlow/Keras"
[//]: # "- PyTorch"
[//]: # "- scikit-learn"
[//]: # "- NLTK"
[//]: # "- pandas, numpy, matplotlib"

---

## ğŸ“ Estructura del Proyecto

```

sentiment-analysis-rnn-lstm-transformer/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ rnn_vs_lstm.ipynb             
â”‚   â””â”€â”€ transformer_exploration.ipynb 
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ rnn_model.keras
â”‚   â”œâ”€â”€ lstm_model.keras
â”‚   â””â”€â”€ transformer_model.pth                           
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

````

---

## ğŸš€ Â¿CÃ³mo ejecutar los notebooks?

1. Clona el repositorio:
```
   git clone https://github.com/tu_usuario/sentiment-analysis-rnn-lstm-transformer.git
   cd sentiment-analysis-rnn-lstm-transformer
```

2. Instala las dependencias:

```
   pip install -r requirements.txt
```

3. Abre y ejecuta los notebooks desde tu entorno favorito:

   * `notebooks/rnn_vs_lstm.ipynb` para la comparaciÃ³n principal
   * `notebooks/transformer_exploration.ipynb` para la arquitectura Transformer educativa

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT.

---

## ğŸ‘¤ Autor

Desarrollado por Alexis MartÃ­nez.
Incluye aprendizaje profundo aplicado a NLP, anÃ¡lisis comparativo de modelos secuenciales y una exploraciÃ³n didÃ¡ctica de la arquitectura Transformer.
